pnorm(4,2,2)
?pnorm
require(graphics)
dnorm(0) == 1/sqrt(2*pi)
dnorm(1) == exp(-1/2)/sqrt(2*pi)
dnorm(1) == 1/sqrt(2*pi*exp(1))
## Using "log = TRUE" for an extended range :
par(mfrow = c(2,1))
plot(function(x) dnorm(x, log = TRUE), -60, 50,
main = "log { Normal density }")
curve(log(dnorm(x)), add = TRUE, col = "red", lwd = 2)
mtext("dnorm(x, log=TRUE)", adj = 0)
mtext("log(dnorm(x))", col = "red", adj = 1)
plot(function(x) pnorm(x, log.p = TRUE), -50, 10,
main = "log { Normal Cumulative }")
curve(log(pnorm(x)), add = TRUE, col = "red", lwd = 2)
mtext("pnorm(x, log=TRUE)", adj = 0)
mtext("log(pnorm(x))", col = "red", adj = 1)
## if you want the so-called 'error function'
erf <- function(x) 2 * pnorm(x * sqrt(2)) - 1
## (see Abramowitz and Stegun 29.2.29)
## and the so-called 'complementary error function'
erfc <- function(x) 2 * pnorm(x * sqrt(2), lower = FALSE)
## and the inverses
erfinv <- function (x) qnorm((1 + x)/2)/sqrt(2)
erfcinv <- function (x) qnorm(x/2, lower = FALSE)/sqrt(2)
pnorm(6,2,2)
pnorm(7,2,2)
pnorm(8,2,2)
pnorm(4,2,2)
dnorm(4,2,2)
dnorm(6,2,2)
dnorm(7,2,2)
dnorm(8,2,2)
norm(0.25,2,2)
norm(0.25,mean=2,sd=2)
qnorm(0.25,mean=2,sd=2)
qnorm(0.75,mean=2,sd=2)
2+(qnorm(0.75,mean=2,sd=2)*2)
pnorm(8.697959,2,2)
pnorm(3.34898,2,2)
x<-rpois(1000,10)
hist(x)
hist(x)
mean(x)
sd(x)
?rpois
set.seed(20)
x<-rnorm(100)
e<-rnorm(100,0,2)
y<-0.5+2*x+e
y
summary(y)
plot(x,y)
x<-bynom(100,1,0.5)
x<-rbinom(100,1,0.5)
y<-0.5+2*x+e
summary(y)
set.seed(10)
x<-rbinom(100,1,0.5)
y<-0.5+2*x+e
set.seed(10)
x<-rbinom(100,1,0.5)
e<-rnorm(100,0,2)
y<-0.5+2*x+e
plot(x,y)
summary(y)
set.seed(1)
x<-rnorm(100)
log.mu<-0.5+0.3*x
y<-rpois(100,exp(log.mu))
summary(y)
plot(x,y)
qnorm(0.025)
pnorm(2.44)
1-pnorm(2.44)
qnorm(0.025)
qnorm(0.95)
pnorm(16,96)
fileUrl<-"http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
rm(e)
fileUrl<-"http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
install.packages("XML")
library(xml)
library("xml")
install.packages("XML")
library(xml)
library("xml")
install.packages("XML")
library(xml)
library(XML)
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode
library(XML)
url<-"http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html<-htmlTreeParse(url,useInternalNodes = T)
xpathSApply(html,"//title",xmlValue)
xpathSApply(html,"//td[@id='col-citedby']",xmlValue)
lista<-xpathSApply(html,"//td[@id='col-citedby']",xmlValue)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
library(httr)
install.packages("httr")
library(httr)
pg1<-GET("http://httpbin.org/basic-auth/user/passwd")
pg1
pg1<-GET("http://httpbin.org/basic-auth/user/passwd",authenticate("user","passwd"))
pg1
names(pg2)
names(pg1)
pg1$url
pg1$headers
pg1$headers$connection
google<-handle("http://google.com")
pg1<-GET(handle = google,path="/")
pg1
pg2<-GET(handle = google,path="search")
pg2
myapp<-oauthapp("twitter",key="owmDZsp01FTFMEJ6biem038p2",secret="ZXvp23KV")
myapp<-oauth_app("twitter",key="owmDZsp01FTFMEJ6biem038p2",secret="ZXvp23KV")
myapp
myapp<-oauth_app("twitter",key="owmDZsp01FTFMEJ6biem038p2",secret="ukLXa0h9XsYqelzBXZtvEVP50E3MJwAWJ6c2iHUe3gZgEZ87r8")
sig<-sign_oauth1.0(myapp,token="475127989-qKOTbcKDRzdPZeGtazFxJEdB0rtDmWEjGWoWQBpQ",token_secret = "zP7ni957mpwzldGrXq3g6lasRswLQZB2jfWeXDI2YUSS5")
homeTL<-GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
installed.packages("base64enc")
homeTL<-GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
library(base64enc)
library(httr)
homeTL<-GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
homeTL<-GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
homeTL
json1<-content(homeTL)
json2<-jsonlite::fromJSON(toJSON(json1))
library(jsonlite)
json2<-jsonlite::fromJSON(toJSON(json1))
json2
json2[1,1:4]
json2[1,1:4]
json2[1,1:4]
json2[2,1:4]
json2[1,1:4]
json1<-content(homeTL)
json2<-jsonlite::fromJSON(toJSON(json1))
json2[1,1:4]
library(httr)
oauth_endpoints("github")
myapp<-oauth_app("github",key="18332495a3806ed7200a",secret = "6282f2a7c2933113e155ed2c64c85251b6aa4d37")
github_token<-oauth2.0_token(oauth_endpoints("github"),myapp)
installed.packages("httpuv")
install.packages("httpuv")
library("httpuv")
github_token<-oauth2.0_token(oauth_endpoints("github"),myapp)
gtoken<-config(token=github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
content(req)
names(content(req))
contenido<-content(req)
contenido$created_at
contenido$owner
contenido[1]
contenido[1]$created_at
contenido[[1]]
contenido[[1]]$created_at
contenido[["datasharing"]]$created_at
contenido[["datasharing"]]
contenido["datasharing"]
summary(contenido)
summary(,"name")
summary(,name)
contenido[,"name"]
contenido[,1:4]
contenido[,1]
contenido[1]$name
contenido[1,]
contenido[1,,]
summary(contenido)
contenido[1]
contenido[[1]]
contenido[[1]]$name
contenido[[2]]$name
class(contenido)
lapply(contenido,function(str) grep("datasharing",ch))
lapply(contenido,function(str) grep("datasharing",str))
contenido[[7]]$name
contenido[[7]]$created_at
install.packages("sqldf")
sapply(contenido,function(str) grep("datasharing",str))
acs<-read.csv("/home/arthur/Descargas/getdata_data_ss06pid.csv")
names(acs)
sqldf("select pwgtp1 from acs where AGEP < 50")
library("sqldf")
sqldf("select pwgtp1 from acs where AGEP < 50")
summary(sqldf("select pwgtp1 from acs where AGEP < 50"))
sqldf("select distinct AGEP from acs")
sqldf("select unique AGEP from acs")
sqldf("select distinct AGEP from acs")
library(httr)
url<-"http://biostat.jhsph.edu/~jleek/contact.html"
page<-GET(url)
con<-url("http://biostat.jhsph.edu/~jleek/contact.html ")
page<-readLines(con)
close(con)
lines<-c(10,20,30,100)
page[1]
nchar(page[lines])
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for")
getdata_wksst8110 <- read.table("~/Descargas/getdata_wksst8110.for", header=TRUE, quote="\"")
View(getdata_wksst8110)
getdata_wksst8110 <- read.table("~/Descargas/getdata_wksst8110.for", header=TRUE, quote="\"")
View(getdata_wksst8110)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=2)
View(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=2,sep=" ",header=TRUE)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=3,sep=" ",header=TRUE)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=4,sep=" ",header=TRUE)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=5,sep=" ",header=TRUE)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=5,header=TRUE)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=3,header=TRUE)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=2,header=TRUE)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=2)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=3)
View(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=4)
View(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=4,sep=" ")
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=4,sep="\t")
View(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,5,8,5,8,5,8,5,8),skip=4,sep="\t")
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,5,8,5,8,5,8,5,8),skip=4)
View(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=4,sep=" ")
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(9,8,8,8,8),skip=4)
View(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(10,8,8,8,8),skip=4)
View(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(10,8,8,8,8),skip=4,sep=" ")
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(10,8,8,8,8),skip=4,sep="\t")
View(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(10,8,8,8,8),skip=4)
head(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(10,5,8,8,8,8),skip=4)
head(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(10,5,8,5,8,5,8,5,8),skip=4)
head(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(10,8,8,8,8),skip=4,strip.white=TRUE)
head(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(-1,9,-5,8,-5,8,-5,8,-5,8),skip=4)
head(data)
data[1]
data[,1]
head(data)
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(1,9,5,8,5,8,5,8,5,8),skip=4)
head(data)
head(data[,4])
head(data[1,4])
data[1,4]
summary(data[,4])
head(data[,4])
View(data)
add(data[,4])
colSums(data[,4])
colSums(data)
eval(parse(data[1,4]))
txt<-"23.4-0.4"
eval(parse(txt))
eval(parse(text=txt))
eval(parse(text=data[1,4]))
eval(parse(text=data[,4]))
class(data)
head(data[[4]])
head(data[[,4]])
head(data[,4])
class(data[,4])
sapply(data,function(str),eval(parse(text=str)))
sapply(data[,4],function(str),eval(parse(text=str)))
lapply(data[,4],function(str),eval(parse(text=str)))
sapply(data[,4],function(str) eval(parse(text=str)))
class(sapply(data[,4],function(str) eval(parse(text=str))))
sum(sapply(data[,4],function(str) eval(parse(text=str))))
data<-read.fwf("/home/arthur/Descargas/getdata_wksst8110.for",widths = c(-1,9,-5,4,-1,3,-5,4,-1,3,-5,4,-1,3),skip=4)
head(data)
sum(data[,4])
set.seed(13435)
X <- data.frame("var1"=sample(1:5),"var2"=sample(6:10),"var3"=sample(11:15))
X <- X[sample(1:5),]
X <- data.frame("var1"=sample(1:5),"var2"=sample(6:10),"var3"=sample(11:15))
X <- X[sample(1:5),]
X$var2[c(1,3)] = NA
x
X
X <- data.frame("var1"=sample(1:5),"var2"=sample(6:10),"var3"=sample(11:15))
set.seed(13435)
X <- data.frame("var1"=sample(1:5),"var2"=sample(6:10),"var3"=sample(11:15))
Y <- X[sample(1:5),]
Y[,1]
X["var1"]
X[,"var1"]
X[1:2,"var2"]
X
Y[1:2,"var2"]
Y$var2[c(1,3)] = NA
Y[1:2,"var2"]
y[(Y$var1<=3 & X$var3>11)]
Y[(Y$var1<=3 & Y$var3>11)]
Y[(Y$var1<=3 & Y$var3>11),]
Y[(Y$var1<=3 | Y$var3>11),]
Y[(Y$var1<=3 | Y$var3>15),]
X<-Y
X
X[X$var2>8,]
?which
X[which(X$var2>8),]
X[sort(X$var1),]
X[order(X$var1),]
order(X$var1)
library(plyr)
install.packages("plyr")
arrange(X,var1)
library(plyr)
arrange(X,var1)
arrange(X,desc(var1))
X$var4<-rnorm(5)
X
X$var4<-rnorm(7)
getwd
getwd()
fileUrl <- "https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl,destfile="restaurants.csv",method="curl")
restData<-read.csv("restaurants.csv")
View(restData)
head(restData)
tail(restData)
head(restData,n=3)
summary(restData)
str(restData)
quantile(restData$councilDistrict)
quantile(restData$councilDistrict,na.rm=TRUE)
quantile(restData$councilDistrict,probs=c(0.5,0.75,0.9))
table(restData$zipCode,useNA = "ifany")
?table
table(restData$zipCode,restData$councilDistrict,useNA = "ifany")
sum(is.na(restData$councilDistrict))
any(is.na(restData$councilDistrict))
all(restData$zipCode>0)
all(restData$zipCode>0)
restData$zipCode>0
restData[restData$zipCode>0,]
head(restData[restData$zipCode>0,],n=3)
head(restData[restData$zipCode==0,],n=3)
head(restData[restData$zipCode=0,],n=3)
head(restData[restData$zipCode<=0,],n=3)
colSums(restData)
colSums(is.na(restData))
all(colSums(is.na(restData))==0)
table(restData$zipCode %in% "21212")
table(restData$zipCode %in% c("21212")
)
table(restData$zipCode="21212")
table(restData$zipCode=="21212")
table(restData$zipCode %in% c("21212","21213"))
restData[restData$zipCode %in% c("21212","21213")),]
restData[restData$zipCode %in% c("21212","21213"),]
head(restData[restData$zipCode %in% c("21212","21213"),],n=3)
data("UCBAdmissions")
DF<-as.data.frame(UCBAdmissions)
str(UCBAdmissions)
str(DF)
summary(DF)
xt<-xtabs(Freq ~ Gender + Admit, data=DF)
xt
DF
xt
warpbreaks$replicate<-rep(1:9,len=54)
xt<-xtabs(breaks ~ .,data=warpbreaks)
xt
ftable(xt)
fakeData(rnorm(1E5))
fakeData<-rnorm(1E5)
object.size(fakeData)
print(object.size(fakeData),units=MB
)
print(object.size(fakeData),units="MB")
require(data.table)
require(plyr)
library(data.table)
library(plyr)
install.packages("data.table")
library(data.table)
library(plyr)
setwd("/home/arthur/Dropbox/Privados/Personales/Cursos/BigData/Get and Clean data/CourseProject")
data.Dir<-"./UCI HAR Dataset"
out.Dir<-"./UCI HAR Dataset/intData"  # Place to write the intgrated data
test.Dir<-"./UCI HAR Dataset/test"    # test files directory
train.Dir<-"./UCI HAR Dataset/train"  # train files directory
data.feat<-fread(paste(data.Dir,"features.txt",sep="/"))        # Read variable names
data.acti<-fread(paste(data.Dir,"activity_labels.txt",sep="/")) # Read activity labels
setkey(data.acti,V1)                                            # Set key to activities
data.Dir<-"./UCI HAR Dataset"
out.Dir<-"./UCI HAR Dataset/intData"  # Place to write the intgrated data
test.Dir<-"./UCI HAR Dataset/test"    # test files directory
train.Dir<-"./UCI HAR Dataset/train"  # train files directory
data.feat<-fread(paste(data.Dir,"features.txt",sep="/"))        # Read variable names
data.acti<-fread(paste(data.Dir,"activity_labels.txt",sep="/")) # Read activity labels
setkey(data.acti,V1)                                            # Set key to activities
test.Xset<-fread(paste(test.Dir,"X_test.txt",sep="/"))  # Read test set
names(test.Xset)<-data.feat$V2                          # Assign column names to test set
cols<-grep("mean|std",colnames(test.Xset))
test.ylab<-fread(paste(test.Dir,"y_test.txt",sep="/"))  # Read test labels
setkey(test.ylab,V1)                                    # Set key to test labels
test.sbj<-fread(paste(test.Dir,"subject_test.txt",sep="/")) # Read test subject ids
# combine data using:
# - sbj: Untouched test subject ids
# - labels: The named activities (merging ylab with the activity names table)
# - The subset columns of Xset that contain "mean" or "std" in the column name
test.full<-data.table(sbj=test.sbj$V1,labels=merge(test.ylab,data.acti,by="V1")$V2
,subset(test.Xset,,cols))
train.Xset<-fread(paste(train.Dir,"X_train.txt",sep="/"))  # Read train set
names(train.Xset)<-data.feat$V2                 # Assign column names to train set
# Select column names that contain "mean" or "std" in the name
cols<-grep("mean|std",colnames(train.Xset))
train.ylab<-fread(paste(train.Dir,"y_train.txt",sep="/"))  # Read train labels
setkey(train.ylab,V1)                                    # Set key to train labels
train.sbj<-fread(paste(train.Dir,"subject_train.txt",sep="/")) # Read train subject ids
train.full<-data.table(sbj=train.sbj$V1,labels=merge(train.ylab,data.acti,by="V1")$V2
,subset(train.Xset,,cols))
# Bind test.full and train.full
listBind<-list(test.full,train.full)
integratedDT<-rbindlist(listaDT)
integratedDT<-rbindlist(listBind)
View(integratedDT)
xt<-xtabs(mean ~ sbj + labels,data = integratedDT)
xt<-xtabs(mean ~ sbj + labels,data = as.data.frame(integratedDT)
)
xt<-xtabs(mean ~ sbj + labels,data = as.data.frame(integratedDT))
xt<-xtabs(mean ~ sbj + labels,data = as.matrix(integratedDT))
